Python
======

# Оглавление

1. <h3>[Настройка системы](#Настройка-системы)</h3>

    - [Работа с несколькими версиями python под windows](#Работа-с-несколькими-версиями-python-под-windows)
    - [Создание portable версии python для windows](#Создание-portable-версии-python-для-windows)
    - [Настройка окружения для разработки на MacOS](#Настройка-окружения-для-разработки-на-MacOS)

2. <h3>[Стандартная библиотека](#Стандартная-библиотека)</h3>
    
    - <h4>[time](#time)</h4>
        
        - [Неожиданности при использовании strptime](#Неожиданности-при-использовании-strptime)
     
    - <h4>[contextlib](#contextlib)</h4>
    
        - [Решение проблемы большого количества вложенных with](#Решение-проблемы-большого-количества-вложенных-with)

3. <h3>[Типы данных](#Типы-данных)</h3>

    - <h4>[Строки](#Строки)</h4>
        
        - [Реализация str в CPython](#Реализация-str-в-CPython)
        - [Вычисление длинны строки в байтах](#Вычисление-длинны-строки-в-байтах)
    
    - <h4>[Целые числа](#Целые-числа)</h4>
    
        - [Реализация int в CPython](#Реализация-int-в-CPython)
    
    - <h4>[Кортежи](#Кортежи)</h4>
    
        - [Реализация tuple в CPython](#Реализация-tuple-в-CPython)
    
    - <h4>[Списки](#Списки)</h4>
        
        - [Реализация list в CPython](#Реализация-list-в-CPython)
        - [Создание строки из списка](#Создание-строки-из-списка)
        - [Разворот списка](#Разворот-списка)
        - [Срезы](#Срезы)

    - <h4>[Словари (dict)](#Словари-dict)</h4>
     
        - [Реализация dict в CPython](#Реализация-dict-в-CPython)  
 
    - <h4>[Файлы](#Файлы)</h4>
    
        - [Кодировка файла](#Кодировка-файла)
        - [Работа с большими файлами](#Работа-с-большими-файлами)
        - [Изменение прав доступа, владельца](#Изменение-прав-доступа,-владельца)
        - [Разбиение файла на несколько генераторов](#Разбиение-файла-на-несколько-генераторов)

4. <h3>[Функции](#Функции-раздел)</h3>

    - <h4>[Стандартные функции](#Стандартные-функции)</h4>
    
        - [Попарное объединение элементов итерируемых объектов](#Попарное-объединение-элементов-итерируемых-объектов)
        - [Перевод из одной системы счисления в другую с помощью int()](#Перевод-из-одной-системы-счисления-в-другую-с-помощью-int())
        - [len()](#Функция-len)
        - [map()](#Функция-map)

    - <h4>[Функции](#Функции)</h4>
    
        - [Особенности использования lambda в цикле](#Особенности-использования-lambda-в-цикле)
        - [Замыкание](#Замыкание)

    - <h4>[Сопрограммы](#Сопрограммы)</h4>
        - [yield from](#yield-from)

5. <h3>[Классы](#Классы)</h3>

    - <h4>[Объектная модель](#Объектная-модель)</h4>
    
        - [Спец атрибуты](#Спец-атрибуты)
        - [Реализация доступа по индексу](#Реализация-доступа-по-индексу)
        - [Протокол итераторов](#Протокол-итераторов)
        - [Отличие `__getattr__` от `__getatrribute__`](#Отличие-__getattr__-от-__getattribute__)
        - [MRO](#MRO)

6. <h3>[Исключения](#Исключения)</h3>

    - [Извлечение traceback](#Извлечение-traceback)
    - [Обработчик для не перехваченных исключений](#Обработчик-для-не-перехваченных-исключений)
    - [Цепочки исключений](#Цепочки-исключений)

7. <h3>[Сборщик мусора](#Сборщик-мусора)</h3>

    - [Как работает сборщик мусора (gc)](#Как-работает-сборщик-мусора-(gc))
    
8. <h3>[Многозадачность](#Многозадачность)</h3>

    - [Сравнение асинхронности и многопоточности. Описание роли GIL во всем этом.](#Сравнение-асинхронности-и-многопоточности-Описание-роли-GIL-во-всем-этом)
    - [В чем отличие конкурентности от многопоточности](#В-чем-отличие-конкурентности-от-многопоточности)
    - [Асинхронность в python](#Асинхронность-в-python)

9. <h3>[Оптимизация и профилирование](#Оптимизация-и-профилирование)</h3>

    - <h4>[Способы измерения производительности](#Способы-измерения-производительности)</h4>
    
        - [Измерение производительности отдельных функций](#Измерение-производительности-отдельных-функций)
        - [Замер времени работы с помощью unix программы time](#Замер-времени-работы-с-помощью-unix-программы-time)
        
    - <h4>[Примеры написания оптимального кода](#Примеры-написания-оптимального-кода)</h4>

        - [Создание списков заполненных определённым числом](#Создание-списков-заполненных-определённым-числом)
        - [Добавление данных в список](#Добавление-данных-в-список)

    - <h4>[Советы по написанию оптимального кода](#Советы-по-написанию-оптимального-кода)</h4>
    
        - [Мелкие классы](#Мелкие-классы)
        - [Поиск (lookup) очень дорогой](#Поиск-(lookup)-очень-дорогой)
        - [Ввод/вывод (python 3.X)](#Ввод/вывод-(python-3.X))
        - [Оптимизация потребления памяти, способы поиска утечек памяти](#Оптимизация-потребления-памяти,-способы-поиска-утечек-памяти)

10. <h3>[Deploy](#Deploy)</h3>
    
    - [Публикация библиотеки в PyPi](#Публикация-библиотеки-в-PyPi)

11. <h3>[Тестирование](#Тестирование)</h3>
    
    - [Основы тестирования](#Основы-тестирования)
    - [Mock](#Mock)

12. <h3>[Разное](#Разное)</h3>

    - [Работа с датой и временем](#Работа-с-датой-и-временем)


<a name='Настройка-системы'></a>
## Настройка системы

<a name='Работа-с-несколькими-версиями-python-под-windows'></a>
### Работа с несколькими версиями python под windows

Начиная с версии Python 3.3 с интерпретатором поставляется специальный 
launcher, который позволяет выбрать запускаемую версию интерпретатора.

Для корректной работы launcher, вы должны быть уверены, что путь к нему 
прописан в переменной окружения PATH.

Если вы выполните в консоли:
```cmd
py
``` 
Будет запущен интерпретатор выбранный по умолчанию.

Скрипту можно указать, какую конкретную версию нужно запустить:
```cmd
py -2.7
```
```cmd
py -3
```

<a name='Создание-portable-версии-python-для-windows'></a>
### Создание portable версии python для windows

1. Заходим на оф. сайт и качаем python версии «embeddable zip file»;
2. Разархивируем скачанный файл;
3. Удаляем из извлеченной папки файл с расширением `_pth`;
4. Создаем папку `lib`, извлекаем туда содержимое файла `<версия_python>.zip` и 
удаляем этот архив;
5. Скачиваем скрипт [https://bootstrap.pypa.io/get-pip.py](https://bootstrap.pypa.io/get-pip.py) и запускаем его при 
помощи скачанного python;
6. Теперь можно при помощи pip установить все нужные библиотеки. 

<a name='Настройка-окружения-для-разработки-на-MacOS'></a>
### Настройка окружения для разработки на MacOS

Данный мануал актуален для компьютеров на основе чипа Apple Silicone. В целом его можно применять и для компьютеров на
intel, но не с небольшими доработками.

1. Устанавливаем brew. Выполнять обе команды требуется для возможности устанавливать как нативно поддерживаемые 
    приложения, так и те, которые будут работать через прослойку совместимости.

    Устанавливает brew для архитектуры x86
    
    ```
    arch -x86_64 /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
    ```
    
    Устанавливает brew для apple silicone
    
    ```
    arch -arm64 /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
    ```
    
    После установки brew для m1 будет установлен в /opt/homebrew/bin/brew, а для intel /usr/local/bin/brew

2. Добавляем в ~/.zshrc и переоткрываем консоль, чтобы настройки применились

    ```bash
    # brew
    if [[ $(arch) == 'arm64' ]]; then
        eval "$(/opt/homebrew/bin/brew shellenv)"
    else
        eval "$(/usr/local/bin/brew shellenv)"
    fi
    export HOMEBREW_NO_ANALYTICS=1
     
    # Poetry
    export PATH="$HOME/.local/bin:$PATH"
     
    # Pyenv
    export PYENV_ROOT="$HOME/.pyenv"
    export PATH="$PYENV_ROOT/bin:$PATH"
    eval "$(pyenv init --path)"
     
    # direnv
    eval "$(direnv hook zsh)"
     
    # Включает погрузку скриптов реализующих автодополнение в консоли
    zstyle ':completion:*:*:git:*' script ~/.zsh/git-completion.bash
    fpath=(~/.zsh $fpath)
    autoload -Uz compinit && compinit
   
    source <(kubectl completion zsh)
     
    # Генерирует флаги необходимые для сборки из исходников сишных библиотек
    export LDFLAGS=""
    export CPPFLAGS=""
    export PKG_CONFIG_PATH=""
     
    pkgs=(curl readline sqlite)
    for pkg in $pkgs; do
        pkg_dir="$HOMEBREW_PREFIX/opt/$pkg"
     
        lib_dir="$pkg_dir/lib"
     
        if [ -d "$lib_dir" ]; then
            export LDFLAGS="$LDFLAGS -L$lib_dir"
        fi
     
        include_dir="$pkg_dir/include"
        if [ -d "$include_dir" ]; then
            export CPPFLAGS="$CPPFLAGS -I$include_dir"
        fi
     
        pkg_config_dir="$lib_dir/pkgconfig"
        if [ -d "$pkg_config_dir" ]; then
            if [ "x$PKG_CONFIG_PATH" = "x" ]; then
                export PKG_CONFIG_PATH="$pkg_config_dir"
            else
                export PKG_CONFIG_PATH="PKG_CONFIG_PATH:$pkg_config_dir"
            fi
        fi
    done 
    ```

3. Открываем консоль для работы с ПО адаптированным под Apple Silicon, чтобы у нас использовалась правильная версия 
    brew и зависимости установились правильно.

    ```
    arch -arm64 zsh
    ```
   
    > P.S. Вы всегда должны загружать оболочку (zsh) для той архитектуры, с ПО для которой вы собираетесь работать.  
    > Если так не делать и просто перед вызовом программы указывать требуемую архитектуру через arch, то вы можете  
    > получить большие проблемы, так как у вас будут загружены переменные окружения указывающие не на те версии 
    > программ.

4. Устанавливаем полезные утилиты необходимые для работы

    ```
    brew install wget direnv bash-completion mc xz python@3.9
    curl https://pyenv.run | bash
    ```

    Устанавливаем удобный терминал

    ```
    brew install iterm2
    ```

5. Устанавливаем docker

    ```
    brew install --cask docker
    ```

6. Устанавливаем poetry

    ```
    curl -sSL https://install.python-poetry.org | python3.9 -
    poetry config virtualenvs.create false
    ```

7. Копируем подсказки для автодоплнения

    ```
    mkdir -p ~/.zsh && cd ~/.zsh
    poetry completions zsh > ~/.zsh/_poetry
    curl -o git-completion.bash https://raw.githubusercontent.com/git/git/master/contrib/completion/git-completion.bash
    curl -o _git https://raw.githubusercontent.com/git/git/master/contrib/completion/git-completion.zsh
    cp $HOMEBREW_PREFIX/share/zsh/site-functions/_docker ~/.zsh/_docker
    cp $HOMEBREW_PREFIX/share/zsh/site-functions/_docker_compose ~/.zsh/_docker_compose
    ```

8. Устанавливаем через pyenv нужную версию python

    ```
    pyenv install 3.10.8
    ```



<a name='Стандартная-библиотека'></a>
## Стандартная библиотека

<a name='time'></a>
### time

<a name='Неожиданности-при-использовании-strptime'></a>
#### Неожиданности при использовании strptime

Иногда можно сталкнуться с неожиданной ошибкой при использовании функции 
strptime, которая заключается в том, что при парсиге строки содержащей в себе
временную зону отличную от установленной сейчас на ПК, возникнет ошибка:

```python
ValueError: time data 'Sat Mar 21 00:00:00 MSK 2015' does not match format '%a %b %d %H:%M:%S %Z %Y'
``` 

Избежать этого можно вот так:

```python
import os
import time

os.environ['TZ'] = 'Europe/Moscow'
print(time.strptime('Sat Mar 21 00:00:00 MSK 2015', '%a %b %d %H:%M:%S %Z %Y'))
```



<a name='contextlib'></a>
### contextlib

<a name='Решение-проблемы-большого-количества-вложенных-with'></a>
#### Решение проблемы большого количества вложенных with

Иногда бывают ситуации, когда необходимо использовать большое количество
вложенных конструкций with или в одной кострукции with необходимо работать с 
несколькими объектами. И в этих случаях, код становится некрасивым, ухудшается  
его читаемость, а количество симвлов в строке переваливает далеко за 80.

Эта ситуация неприятная и ее хочется как-то решить. К счатью, в стандартной 
библиотеке есть класс `contextlib.ExitStack`, который решает описанную выше 
проблему.

Для того, чтобы показать как работать с ним, предположим, что у нас есть 2 
класса: Foo и Spam.

```python
class Foo:
    def __enter__(self):
        print('enter Foo')
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        print('exit Foo')

    def work(self):
        print('work Foo')


class Spam:
    def __enter__(self):
        print('enter Spam')
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        print('exit Spam')

    def work(self):
        print('work Spam')
```
 
При классическом подходе работа с ними будет выглядеть так:

```python
with Foo() as foo, Spam() as spam:
    foo.work()
    spam.work()
```

Но, при использовании `ExitStack`, код выше можно переписать следующим образом:

```python
import contextlib

with contextlib.ExitStack() as stack:
    foo = stack.enter_context(Foo())
    spam = stack.enter_context(Spam())
    foo.work()
    spam.work()
```


>Чаще всего, проблема с большим количеством вложенных with встречается при 
>работе c асинхронным кодом. По этому, для работы с асинхронными менеджерами 
>контекста есть специальный класс, который назвается `contextlib.AsyncExitStack`. 


Полезные ссылки:

- [contextlib - ExitStack](https://docs.python.org/3.7/library/contextlib.html#contextlib.ExitStack)
- [contextlib - AsyncExitStack](https://docs.python.org/3.7/library/contextlib.html#contextlib.AsyncExitStack)

<a name='Типы-данных'></a>
## Типы данных

<a name='Строки'></a>
### Строки

<a name='Реализация-str-в-CPython'></a>
#### Реализация str в CPython

[Реализация строкового типа в CPython](https://habr.com/ru/post/480324/), [копия](https://cloud.mail.ru/public/Dzw2/jZ3H5z5Rz)


<a name='Вычисление-длинны-строки-в-байтах'></a>
#### Вычисление длинны строки в байтах

Для того, чтобы вычислить сколько байтов занимает строка нужно:
```python
text = 'Какая-то строка'
print(len(text.encode('utf8')))
```


<a name='Целые-числа'></a>
### Целые числа


<a name='Реализация-int-в-CPython'></a>
#### Реализация int в CPython

[Реализация целого типа в CPython](https://habr.com/ru/post/455114/), [копия](https://cloud.mail.ru/public/dLCi/4pPxztZsT)


<a name='Кортежи'></a>
### Кортежи


<a name='Реализация-tuple-в-CPython'></a>
#### Реализация tuple в CPython

В python кортежи имеют такую же реализацию как и списки, но с дополнительными
оптимизациями. Оптимизации основаны на том, что кортежи являются не изменяемым
типом данных и имеют фиксированный размер.

Во первых, нельзя создать несколько разных объектов содержащих пустой кортеж.
Python однажды создает объект содержащий пустой кортеж и потом использует его
на всем протяжении жизни программы.

Далее, чтобы избежать постоянного выделения памяти под маленькие кортежи. 
Python может переиспользовать созданные ранее, которые больше не 
используются в программе и имеют "малый" размер. Определение размера 
производится на основании константы `PyTuple_MAXSAVESIZE`. Таким образом все
кортежи чья длинна не превышает значение константы `PyTuple_MAXSAVESIZE`
могут быть сохранены в специальный список и потом переиспользованы. 

Сам список хранящий объекты кортежей для повторого использования представляет 
из себя массив, который имеет длинну `PyTuple_MAXSAVESIZE`, где каждый n-й
элемент массва указывает либо на NULL (если нет кортежей размера n, которые
можно переиспользовать), либо на кортеж размеро n, который можно 
переиспользовать. Если имеется несколько кортежей одинаковой длинны, которые 
могут быть переиспользованы, то они объединяются в связанный список.
Для того, чтобы гарантировать, что не будет сохранено слишком много объектов и
они не займут всю память, существует вторая константа `PyTuple_MAXFREELIST`,
которая контролирует максимальную длинну любого из этих связанных списков, в 
любом элементе массива. Так же есть ещё один массив размера 
`PyTuple_MAXSAVESIZE`, который хранит длинну связанных списков состоящих из
кортежей. Это необходимо для контроля заданных ограничений. 

Использованные материалы:
- [Understanding Python's underlying data structures](http://blackecho.github.io/blog/programming/2016/03/23/python-underlying-data-structures.html)
- [How is tuple implemented in CPython?](https://stackoverflow.com/questions/14135542/how-is-tuple-implemented-in-cpython)


<a name='Списки'></a>
### Списки

<a name='Реализация-list-в-CPython'></a>
#### Реализация list в CPython

В Python списки реализованы как массив хранящий ссылки на объекты типа 
`PyObject`. Такая реализация позволяет обращаться к элементу массива по 
индексу за констатное время. Увеличение размера массива происходит по 
определенному алгоритму ([исходный код](https://hg.python.org/cpython/file/tip/Objects/listobject.c)).
Вот как будет увеличиваться размер массива при последовательном добавлении в
него новых элементов: 0, 4, 8, 16, 25, 46, 58, 72, 88, and etc.

Использование источники:

- [Understanding Python's underlying data structures](http://blackecho.github.io/blog/programming/2016/03/23/python-underlying-data-structures.html)
- [Внутреннее устройство Python list](https://habr.com/post/273045/)




<a name='Создание-строки-из-списка'></a>
#### Создание строки из списка

Свернуть список можно банально использовав цикл:

```python
numbers = ['1', '2', '3', '11']
result = 1
for number in numbers:
    result += number
```

или можно красиво сделать все это в 1 строчку с помощью функции reduce

```python
from functools import reduce
a = ['1', '2', '3', '11']
print(reduce(lambda x1, x2: x1 + ', ' + x2, a))
```

или

```python
a = ['1', '2', '3', '11']
', '.join(a)
```


<a name='Разворот-списка'></a>
#### Разворот списка

Развернуть список можно 2 способами:

```python
print(reversed([1, 2, 3, 4, 5]))
```

или

```python
print([1, 2, 3, 4, 5][::-1])
```


<a name='Срезы'></a>
#### Срезы

Очень часто, надо получить не один какой-то элемент, а некоторый их набор 
ограниченный определенными простыми правилами — например первые 5 или 
последние три, или каждый второй элемент — в таких задачах, вместо перебора 
в цикле намного удобнее использовать так называемый срез (slice, slicing).

Следует помнить, что взяв элемент по индексу или срезом (slice) мы не как 
не меняем исходную коллекцию, мы просто скопировали ее часть для дальнейшего 
использования (например добавления в другую коллекцию, вывода на печать, 
каких-то вычислений). Поскольку сама коллекция не меняется — это применимо 
как к изменяемым (список) так и к неизменяемым (строка, кортеж) 
последовательностям.

Синтаксис среза похож на таковой для индексации, но в квадратных скобках 
вместо одного значения указывается 2-3 через двоеточие:

```
# старт, стоп и шаг
my_collection[start:stop:step]
```

**Особенности работы со срезами**

- Отрицательные значения старта и стопа означают, что считать надо не с 
начала, а с конца коллекции.
- Отрицательное значение шага — перебор ведём в обратном порядке справа налево.
- Если не указан старт [:stop:step]— берём с самого начала коллекции, 
то есть start = 0
- Если не указан стоп [start:: step] — идем до самого конца коллекции, т
о есть stop = 0
- step = 1, то есть последовательный перебор слева направо указывать 
не обязательно — это значение шага по умолчанию. В таком случае 
достаточно указать [start:stop]
- Можно сделать даже так [:] — это значит взять коллекцию целиком
- ВАЖНО: При срезе, первый индекс входит в выборку, а второй нет! То есть от 
старта включительно, до стопа, где стоп не включается в результат. 
Математически это можно было бы записать как [start, stop)

**Именованные срезы**

Чтобы избавится от «магических констант», особенно в случае, когда один и 
тот же срез надо применять многократно, можно задать константы с именованными 
срезами с пользованием специальной функции slice().

Примечание: Nonе соответствует опущенному значению по-умолчанию. 
То есть [:2] становится slice(None, 2), а [1::2] становится slice(1, None, 2).

```python
# задаем константам именованные срезы
>>> NAME, BIRTHDAY = slice(None, 2), slice(2, None)
>>> person = ('Alex', 'Smith', "May", 10, 1980)
>>> person[NAME]
('Alex', 'Smith')
>>> person[BIRTHDAY]
('May', 10, 1980)
```

**Изменнеие списка срезом**

Добавление элементов:

```python
>>> a = [1, 2, 3, 4, 5] 
>>> a[1:1] = ['доп. элемент1', 'доп. элемент2'] 
>>> a 
[1, 'доп. элемент1', 'доп. Элемент2', 2, 3, 4, 5]
```

Замена элементов:
```python
>>> a = [1, 2, 3, 4, 5] 
>>> a[2:] = [10, 15, 20] 
>>> a 
[1, 2, 10, 15, 20]
```

Удаление элементов:

```python
>>> a = [1, 2, 3, 4, 5] 
>>> a[2:] = [] 
>>> a 
[1, 2]
```


<a name='Словари-dict'></a>
### Словари (dict)


<a name='Реализация-dict-в-CPython'></a>
#### Реализация dict в CPython

>Актуально для python 3.5 и ниже 

Словари - это неупорядоченные коллекции произвольных объектов с доступом по 
ключу. Их иногда ещё называют ассоциативными массивами или хеш-таблицами.

Перед тем как реализовывать словари в python, разработчики интепретатора 
проанализировали для чего они чаще всего используются в интерпретаторе и не 
только. 

Ниже представлена статистика, которую они собрали:

- именованные параметры функций
    - 1 запись
    - 1-3 элемента
    - часто используется в простых программах
- поиск метода в классе (все методы лежат в словаре, который доступен через 
атрибут `__dict__`) 
    - 1 запись, много чтений
    - 8-16 элементов
    - при наследовании много неудачных чтений с последующим поиском в базовом
    классе
- атрибуты и глобальные переменные
    - много записей и чтений
    - 4-10 элементов
- builtins (все встроенные функции)
    - частые чтения, почти не бывает записи
    - ~150 строковых ключей (python 3.3)
    - по некоторым ключам чтения гораздо чаще чем по другим
- удаление повторов, подсчет элементов
    - однократное чтение по каждому из ключей
    - произвольное количество элементов
    - многократный доступ по одному ключу подрят
- проверка принадлежности
    - словари произвольных размеров
    - создаются 1 раз и затем мало изменяются
    - много вызовов `has_key()` и `__contains__`
-  динамическое отображение (то есть происходит постоянная работа со словарем:
чередующиеся добавления, удаления, чтение и перезапись элементов) 

Примеры

- Удаление дубликатов

    ```python
    dict.fromkeys(seqn).keys()
    ```

    Все опреации записи выполняются при конструировании.

- Подсчет элементов в последовательности

    ```python
    for i in seq:
        d[i] = d.get(i, 0) + 1  
    ```
    
    Выполняется 2 последовательных доступа по ключу и используя метод get мы
    не можем уменьшить количество выполняемых операций.
 
- Создание индекса из словаря списков
    
    ```python
    for page_index, page in enumerate(pages):
        for word in page:
            d.setdefault(word, []).append(page_index)
    ```
    
    setdefault сомещает 2 поиска в одном (в отличии от get).


Как видно из статистики, разработки интерпретатора рассмотрели множество 
вариантов в которых могут использоваться словари и постарались сделать 
максимально универсальную реализацию, оптимизации специфических случаев 
практически не делались.

**Внутренняя реализация в python 2.7**

Расматривается реализация словарей из python 2.7, потому что она достаточно 
проста для понимания и отличий от реализации в python 3.3 не так много.

Словарь это просто последовательная область памяти (массив). Для реализации
был выбран массив, потому что доступ к его элементам максимально быстрый.
В Си коде массив выглядит как массив содержаший струкртуры вида:

```с
typedef struct {
    Py_hash_t me_hash;
    PyObject *me_key;
    PyObject *me_value;
} PyDictKeyEntry;
```  

Описание полей стуктуры:

- me_hash - хэш
- *me_key - указатель на объект, который является ключом
- *me_value - указатель на объект, который является значеним

Легко прикинуть сколько данная структура занимает в места в пяти. Хэш занимает
32/64 бита (в зависимости от разрядности). То есть на 32 битной системе такая 
стуктура будет занимать 12 байт.

Новый словарь в памяти выглядит следующим образом

![](python-notes-index/dict/impl-dict-in-cpython/1.jpg)

> *Колонка Idx явно не присутвует и нужна только для удобства восприятия.*

При инициадизации словаря (python 2.7 и 3.3) в нем сразу выделяется место для 8 
значений, это делается для оптимизации. Интересно отметить, что хоть место и
выделено для 8 значений, но 8 элементов в нем храниться не может (это будет 
объяснено далее).

В словаре в качестве ключа может выступать любой НЕ изменяемый объект (строка,
кортеж, frozen set и т д), в том чисте и кастомный объект в котором 
переопределен метод `__hash__`. 

**Добавление нового элемента в словарь**

Для того, чтобы добавить новый элемент в словарь нужно преобразовать ключ в 
индекс массива (таблицы). Это делается следующим образом:

- ключ преобразововывается в число при помощи функции hash.
- у полученного числа берется n младших бит 

Полученное значение и есть необходимый нам индекс массива.

> Особенность массива (таблицы) в том, что ее размер это всегда степень двойки, 
> по этому n бит всегда будет валидным ключом, где n это степень двойки.

Немного подробностей о работе функции hash.

Функция `hash` это просто builtin функция возвращающая 32/64 битное значение. 
Для вычисления возвращаемого значения используется системная функция, либо
метод `__hash__`, если он переопределен.

Генерируемый хеш должен обладать следующими свойствами:

- для одинаковых значений он всегда должен быть одинакомым
- хеш должен быть один и тот же, даже если представления разные
типами (9, 9.0, complex(9,0))
- похожие значения должны давать сильно отличающиеся хеши

Теперь рассмотрим на примерах как происходит добавление новых элементов в 
словарь:

> Функция bits это не какая-то стандартная функция, она просто выводит
> бинарное представление числа, в данном примере используется только для
> визуализации.

> Ещё необходимо понимать, что в действительности в таблице хранится указатель
> на объект python`а, а не как изображено.

<details markdown="span">
    <summary>Показать слайды</summary>
    
![](python-notes-index/dict/impl-dict-in-cpython/add-new-element-1.jpg)
![](python-notes-index/dict/impl-dict-in-cpython/add-new-element-2.jpg)
![](python-notes-index/dict/impl-dict-in-cpython/add-new-element-3.jpg)
![](python-notes-index/dict/impl-dict-in-cpython/add-new-element-4.jpg)
![](python-notes-index/dict/impl-dict-in-cpython/add-new-element-5.jpg)
![](python-notes-index/dict/impl-dict-in-cpython/add-new-element-6.jpg)
![](python-notes-index/dict/impl-dict-in-cpython/add-new-element-7.jpg)
![](python-notes-index/dict/impl-dict-in-cpython/add-new-element-8.jpg)
![](python-notes-index/dict/impl-dict-in-cpython/add-new-element-9.jpg)
![](python-notes-index/dict/impl-dict-in-cpython/add-new-element-10.jpg)

</details>

На данных слайдах изображен идеальный случай заполнения словаря, потому что
у нас не возникло коллизий (не было ситуаций, когда для разных значений
вычисленный хэш был одинаковым).

**Поиск значений в словаре**

На данный момент у нас есть заполненный словарь и мы хотим извлечь из него 
интересующие нас значения. Рассмотрим как это реализуется:

- сначала мы вычисляем хэш от ключа
- обрезаем старшие биты
- берем полученное значение и используем его как индекс для доступа к элементам
  массива
  
Если визуализировать, то выглядит это следующим образом:

![](python-notes-index/dict/impl-dict-in-cpython/search-element.jpg)


**Перебор всех элементов словаря**

Получение всех элементов словаря: при итерировании, получение ключей `keys()` 
или получении значений `values()`, реализуется как последовательный перебор 
всех элементов таблицы, где пустые элементы пропускаются. Таким образом 
если распечатать словарь, то элементы в нем будут расположены в том порядке, в 
котором они лежат в памяти. Это совсем не тот порядок, в котором происходило 
добавление элементов в словарь.

<details markdown="span">
    <summary>Показать слайды</summary>
    
![](python-notes-index/dict/impl-dict-in-cpython/print-dict-1.jpg)
![](python-notes-index/dict/impl-dict-in-cpython/print-dict-2.jpg)
![](python-notes-index/dict/impl-dict-in-cpython/print-dict-3.jpg) 

</details>

**Коллизии**

Во всех предыдущих примерах, когда мы наблюдали за добавлением новых элементов
в словарь, рассматривался идеализированный случай, потому что при добавлении
новых элементов ни когда не возникало коллизий.

Коллизия - это ситуация, когда при вычислении индекса таблицы, для двух разных
ключей будет будет сгенерирован одинаковый индекс.

Примером может служить ситуация, когда хэш от двух разных ключей одинаковый. 
Причиной этого является особенность реализации функции `hash`, так как она 
расчиана на скорость, а не криптостойкость, а также то, что при вычислении 
индекса берется только n младших бит хэша.

Есть несколько способов как действовать при возникновении коллизии, в python 
используется метод "Открытой адресации". Его суть заключается в том, что после
возникновения коллизии мы находим в таблице первое свободное место после 
занятой ячейки и записываем элемент туда. Важно заметить, что вычисление 
индекса следующей свободной ячейки происходит не при помощи увеличения текущего
индекса на единицу. Как это делается будет рассмотрено позже, сейчас рассмотрим
как в общих чертах работает алгоритм "Открытой адресации".

<details markdown="span">
    <summary>Показать слайды</summary>
    
![](python-notes-index/dict/impl-dict-in-cpython/collision-example-1.jpg)
![](python-notes-index/dict/impl-dict-in-cpython/collision-example-2.jpg)
![](python-notes-index/dict/impl-dict-in-cpython/collision-example-3.jpg)
![](python-notes-index/dict/impl-dict-in-cpython/collision-example-4.jpg)
![](python-notes-index/dict/impl-dict-in-cpython/collision-example-5.jpg)
![](python-notes-index/dict/impl-dict-in-cpython/collision-example-6.jpg)
![](python-notes-index/dict/impl-dict-in-cpython/collision-example-7.jpg)
![](python-notes-index/dict/impl-dict-in-cpython/collision-example-8.jpg)
![](python-notes-index/dict/impl-dict-in-cpython/collision-example-9.jpg)
![](python-notes-index/dict/impl-dict-in-cpython/collision-example-10.jpg)
![](python-notes-index/dict/impl-dict-in-cpython/collision-example-11.jpg) 

</details>

На слайдах выше видно, как происходило добавление новых элементов, как 
разрешались коллизии (элементы, при добавлении которых произошла коллизия 
отмечены красным перечеркнутым равенством). Как можно заметить у элементов
при добавлении, которых произошла коллизия 3 младшие бита хэша не совпадают с
индексом ячейки в которой они хранится. Эта информация будет использоваться в
дальнешем при поиске элементов по ключу.


**Алгоритм поиска первой свободной ячейки**

Как говорилось ранее, алгоритм поиска первой свободной ячейки не просто 
увеличивает текущий индекс на единицу. Это связано с тем, что последовательный
поиск очень не оптимален, если ключами словаря служат числа. Дело в том, что 
хэш от числа это само число (если число больше 32 битов, на 32 битной системе,
то просто отбрасываются старшие биты). А на практике довольно часто 
встречаются словари, в которых ключи это последовательные числа. Соотвественно,
если бы алгоритм поиска свободной ячейки просто увеличивал индекс на единицу,
то при добавлении в такой словарь нового элемента, ключ у которого строка 
и ее хэш попадал бы на начало последовательности, то нам бы пришлось 
пройти всю таблицу, чтобы найти первую свободную ячейку.

Для того, чтобы избежать такой ситуации была выбран следующий алгоритм:

```c
# pertrurb нужен для того, чтобы сделать генерируемое значение более случайным
pertrurb = hash
while (<слот занят>) {
    perturb >>= 5;
    # slot хранит предыдущие значение
    slot = (5 * slot) + 1 + perturb;
}
```

Если вызывать данный аглоритм 2 в 32 степени раз, то он сгенерирует 
последвательность всех int`отв, только они будут не отсортированы.


**Получение значений по ключу**

Пример получения значения по ключу, простой случай:

![](python-notes-index/dict/impl-dict-in-cpython/get-value-by-key-1.jpg)

В простом случае получение значения по ключу происходит по стандароному 
алгоритму:

- считаем хэш от ключа
- берем от хэша n младших битов
- полученое значение используем как индекс
- готово, нужное значение получили

В сложном случае мы можем столкнуться с коллизией:

![](python-notes-index/dict/impl-dict-in-cpython/get-value-by-key-2.jpg)

В этом случае алгоритм будет следующий:

- считаем хэш от ключа
- берем от хэша n младших битов
- полученое значение используем как индекс

TODO дописать https://www.youtube.com/watch?v=JhixzgVpmdM 20:55 

>Данная заметка основана на докладе Кирилла Лашкевича 
>[Dictionary в Python. Кирилл Лашкевич - Python Meetup 30.08.2013](https://www.youtube.com/watch?v=JhixzgVpmdM), ([Локальная копия](https://cloud.mail.ru/public/3ph9/3LvBrTk5o))

Материалы по данной теме:

- [Как реализован Python в словарях | qaru](http://qaru.site/questions/23060/how-are-pythons-built-in-dictionaries-implemented)
- [Как реализованы встроенные словари python | rupython](http://www.rupython.com/147-147.html)


>Python 3.6 и выше

Начиная с python 3.6 реализация словарей была изменена. Давайте рассмотрим 
реализаю современных словарей.

До версии 3.6 все данные хранились в одном достаточно сильно разряженом 
массиве (прочитать деталии реализации можно в начале статьи), а начиная с 
версии 3.6 для хранения данных решено было использовать 2 массива:
 
- первый массив хранит указатели на стуктуры `PyDictKeyEntry`
    ```c
    typedef struct {
        Py_hash_t me_hash;
        PyObject *me_key;
        PyObject *me_value;
    } PyDictKeyEntry;
    ```
  Стоит заметить, что элементы в данном массиве хранятся в порядке их 
  добавления и записываются последовательно (в массиве отсуствуют пустосты);
- второй массив хранит индексы указывающие на данные из первого массива. Данный
  массив уже имеет разряженную структуру (как в изначальной реализации).

Рассмотрим пример написанный на python. Для визуализации возьмем следующий 
словарь:

```python
d = {'timmy': 'red', 'barry': 'green', 'guido': 'blue'}
```

До версии 3.6 данные такого словаря хранились вот так:

```python
entries = [['--', '--', '--'],
           [-8522787127447073495, 'barry', 'green'],
           ['--', '--', '--'],
           ['--', '--', '--'],
           ['--', '--', '--'],
           [-9092791511155847987, 'timmy', 'red'],
           ['--', '--', '--'],
           [-6480567542315338377, 'guido', 'blue']]
```

А начиная с версии 3.6 и выше:

```python
indices =  [None, 1, None, None, None, 0, None, 2]
entries =  [[-9092791511155847987, 'timmy', 'red'],
            [-8522787127447073495, 'barry', 'green'],
            [-6480567542315338377, 'guido', 'blue']]
```

Как мы видим из данного примера изменился только способ хранения данных и он 
ни как не влияет на алгоритмы работы заложеные в раннюю реализацию словарей.


Материалы по данной теме:

- [Современные словари в python: сочетание дюжины отличных идей. Реймонд Хеттинджер - PyCon 2017](http://pythonz.net/videos/95/), [Копия доклада](https://cloud.mail.ru/public/3DTb/3RiKYo5c2)
- [Реализация словарей из python >= 3.6 написанная на python](python-notes-index/dict/impl-dict-in-cpython/impl-compact-and-ordered-dict.py) - 
  код из презентации Реймонда Хеттинджера
- [Proof-of-concept for a more space-efficient, faster-looping dictionary](http://code.activestate.com/recipes/578375/)
- [Faster, more memory efficient and more ordered dictionaries on PyPy](https://morepypy.blogspot.com/2015/01/faster-more-memory-efficient-and-more.html)
- [Are dictionaries ordered in Python 3.6+? | StackOverflow](https://stackoverflow.com/questions/39980323/are-dictionaries-ordered-in-python-3-6)
- [Реализация словаря в Python - habr](https://m.habr.com/ru/company/otus/blog/448350/)
- [Немного внутренностей словарей в CPython (и PyPy)](https://habr.com/ru/post/432996/), [копия](https://cloud.mail.ru/public/5J7t/2tUtjwNpD)


<a name='Файлы'></a>
### Файлы


<a name='Кодировка-файла'></a>
#### Кодировка файла

При открытие файла лучше всего всегда явно указывать его кодировку. 
В python3 это делается это вот-так:

```python
file = open('Титульник.txt', encoding='utf-8')
```

В python2, нужно использовать функцию open из модуля codecs:

```python
import codecs
codecs.open('Титульник.txt', encoding='utf-8')
```

Чтобы работать с файлом в кодировке utf8-bom, его нужно открывать 
следующим образом:

python3

```python
inp = open('test', encoding='utf-8-sig')
```

python2
```python
import codecs
inp = codecs.open('test', encoding='utf-8-sig')
```

<a name='Работа-с-большими-файлами'></a>
#### Работа с большими файлами

Правильный подход при работе с большими файлами состоит в том, чтобы не 
загружать в память файл за раз целиком (например методом `readlines()`), а 
считывать файл постепенно:

- `read(n)` позволяет считать заданное количество байт из файла;
- `readline()` считывает за раз только одну строку из файла;
- так-же можно использовать протокол итераторов.

В python протоколом итераций называют такое поведение, когда объект реализует 
метод `__next__` который возврщает при вызове следующие значение, а когда 
объекты кончились возбуждает исключение `Stoplteration`.
Файлы поддерживают итерационный протокол (имееют метод `__next__`), по этому 
правильным вариантом работы с ними будет использование цикла for, так как он 
при своей работе вызывает метод объекта `__next__`, так же он сам перехватывает 
исключение `Stoplteration`.

Пример:

```python
for line in open('file.txt'):
    print(line.upper())
```

Метод `__next__` можно вызывать и ручную используя цикл `while`, но в этом 
случае нужно в ручную перехватывать исключение `StopIteration`.

```python
inp = open('out')
while True:
    try:
        print(inp.__next__())
    except StopIteration:
        break
```

<a name='Изменение-прав-доступа,-владельца'></a>
#### Изменение прав доступа, владельца

**Изменение прав доступа**

```python
import os
script_update = '/home/alex/update.sh'
os.chmod(path=script_update, mode=int('700', base=8))
```

Агрумент mode принимает значение в восьмеричном формате, по этому десятиричное 
представление нужно преобразовывать, перед передачей функции.

**Изменение владельца**

```python
import os
script_update = '/home/alex/update.sh'
os.chmod(path=script_update, mode=int('700', base=8))
```

<a name='Разбиение-файла-на-несколько-генераторов'></a>
#### Разбиение файла на несколько генераторов

```python
import math
import itertools

def openfile(filename):
   with open(filename) as file_:
       for i in file_:
           yield i.strip()

def file_split(filename, parts):
   """
   Разбивает содержимое файла на несколько итераторов
   """
   all_str = sum(1 for i in open(filename, 'r'))  # кол-во строк в файле
   count = math.ceil(all_str / parts)  # размер среза

   offset = 0
   chunks = []  # Итераторы содержащие разные части файла
   for i in range(parts):
       file_ = openfile(filename)
       chunks.append((itertools.islice(file_, offset, offset + count)))
       offset += count

   return chunks
```



<a name='Функции-раздел'></a>
## Функции


<a name='Стандартные-функции'></a>
### Стандартные функции


<a name='Попарное-объединение-элементов-итерируемых-объектов'></a>
#### Попарное объединение элементов итерируемых объектов

Из двух (или более) итерируемых последовательностей можно создать один список, 
содержащий кортежи из пар элементов этих последовательностей, а недостающие 
будут заменены None.

В python2, для этого можно использовать стандартную функцию map:

```python
>>> map(None, [1, 2, 3], [1, 2]) 
[(1, 1), (2, 2), (3, None)]
```

В python3, нужно использовать функцию zip_longest из модуля itertools:

```python
>>> from itertools import zip_longest
>>> list(zip_longest([1, 2, 3], [1, 2], fillvalue=None))
[(1, 1), (2, 2), (3, None)]
```


<a name='Перевод-из-одной-системы-счисления-в-другую-с-помощью-int()'></a>
#### Перевод из одной системы счисления в другую с помощью int()

В Python, чтобы перевести число из одной системы счисления в другую не нужно 
городить свои велосипеды, можно воспользоваться встроенной функцией int.

```python
>>> int("AF", 16)
175 
```

В этом примере мы перевели шестнадцатиричное число в десятичную систему 
счисления. Для этого мы в качестве 1 аргумента передали в виде строки число 
в шестнадцатеричной системе счисления и 2 аргументом передали основание 
системы счисления (Основание системы счисления, показывает сколько цифр и 
символов применяется для изображения числа. Например в десятичной системе 
счисления, основание будет 10.).


<a name='Функция-len'></a>
#### len

len - возвращает число элементов в указанном объекте-контейнере.

Для того, чтобы данная функция могла работать с экземлярами классов, которые 
разработали вы сами, необходимо реализовать в них метод `__len__`.

>При работе со встроенными типами данных (str, list, bytearray) len возвращает 
>значение поля `ob_size` C-структуры `PyVarObject`, которая представляет в 
>памяти любой встроенный объект. Благодя такой реализации, функция `len` очень
>быстро работает со встроенными типами, так как обращение к полю тратит меньше 
>ресурсов, чем вызов метода.


<a name='Функция-map'></a>
#### map

Функция map применяет функцию к каждому элементу последовательности и возвращает итератор с результатами.

Например, с помощью map можно выполнять преобразования элементов. Перевести все строки в верхний регистр:

```python
>>> list_of_words = ['one', 'two', 'list', '', 'dict']
>>> map(str.upper, list_of_words)
<map object at 0x102378760>
>>> list(map(str.upper, list_of_words))
['ONE', 'TWO', 'LIST', '', 'DICT']
```

**Производительность**

Функция map работает в некоторых случаях немного быстрее чем list comprehensions:

```bash
alex@macbook PycharmProjects % python3 -m timeit -u nsec -s'xs=range(10)' 'list(map(hex, xs))'
1000000 loops, best of 5: 404 nsec per loop
alex@macbook PycharmProjects % python3 -m timeit -u nsec -s'xs=range(10)' '[hex(x) for x in xs]'
500000 loops, best of 5: 533 nsec per loop
```

Но не в том случае, если для преобразования последовательности применяется lambda:

```bash
alex@macbook PycharmProjects % python3 -m timeit -u nsec -s'xs=range(10)' 'list(map(lambda x: x+2, xs))'
500000 loops, best of 5: 557 nsec per loop
alex@macbook PycharmProjects % python3 -m timeit -u nsec -s'xs=range(10)' '[x+2 for x in xs]'
1000000 loops, best of 5: 350 nsec per loop
```

P.S. Данные тесты производились на Python 3.9.7, на macbook pro 13 c m1.

Полезные ссылки:

- [List comprehension vs map - stackoverflow](https://stackoverflow.com/a/1247490)



<a name='Функции'></a>
### Функции

<a name='Особенности-использования-lambda-в-цикле'></a>
#### Особенности использования lambda в цикле

В цикле анонимные функции нужно использовать с осторожностью, потому что 
они «ленивые». По другому это называется отложенные вычисления. 

> Отложенные вычисления (англ. lazy evaluation, также ленивые вычисления или 
> нестрогие вычисления) — технология, которая позволяет вам отсрочить 
> вычисление кода до тех пор, пока не понадобится получить результат 
> вычислений.

```python
args = [1, 2, 3]
for i in args:
    func = lambda x: x + i  # 3
    print(func(0))  # 4

funcs = []
for j in args:
    funcs.append(lambda x: x + j)
for f in funcs:  # 9
    print(f(0))
```

Пояснения:

- 3 определили функцию, но пока мы ее не вызовем, переменные внутри функции 
не будут инициализированны;
- 4 тут все ожидаемо, тело функции было инициализированно с текущим значением i;
- 9 в этом цикле при вызове любой функции будет выведено число 3. Это 
получается потому, что инициализация lambda функции происходит при её вызове 
(это как раз и есть ленивые вычисления, вычисления происходят только тогда, 
когда это требуется) и при вызове функции перменная j == 3 .

**Вывод**

Анонимные функции инициализируется непосредственно в момент их вызова и если 
значение переменной которая в неё передавалась изменилось, то функция будет 
инициализированна с тем значением которое в ней записано на момент вызова, а 
не с тем, какое передавалось в функцию при создании. В приведённом примере 
выход из цикла происходит, когда j == 3 и получается, что во всех созданных 
функциях j будет равно 3.

**Решение сложившейся проблемы**

Описанную выше проблему можно избежать если изменить код и сделать все так:

```python
funcs = []
for i in range(1, 4):
   funcs.append(lambda arg1, arg2=i: arg1 + arg2)
for f in funcs:
   print(f(1))
```


<a name='Замыкание'></a>
#### Замыкание

Замыкание (англ. closure) — функция первого класса, в теле которой присутствуют
ссылки на переменные, объявленные вне тела этой функции в окружающем коде и 
не являющиеся её параметрами.

Замыкание, также как и экземпляр класса, есть способ представления 
функциональности и данных, связанных и упакованных вместе.

В жизни то, что описано выше выглядит следующим образом:

```python
>>> def foo(a):
...     def bar(b):
...         return b * a 
...     return bar
... 
>>> bar = foo(2)
>>> bar(5)
10
>>> bar(10)
20
```

Как мы видим, функция bar может получать доступ к переменным из области 
видимости foo, это и называется замыканием.



<a name='Сопрограммы'></a>
### Сопрограммы

<a name='yield-from'></a>
#### yield from

yield from можно рассматривать с 2 сторон:

1. Это синтаксический сахар

    Реализация без yield from
    
    ```python
    def generator():
        for i in range(10):
            yield i
    
    def wrapper():
        for i in generator():
            yield i
    
    for i in wrapper():
        print(i)
    ```
    
    Реализация c и спользованием yield from
    
    ```python
    def generator():
        for i in range(10):
            yield i
    
    def wrapper():
        yield from generator()
    
    for i in wrapper():
        print(i)
    ```

2. yield from обеспечивает прозрачный двусторонний канал от вызывающего до 
субгенератора. Это включает получение данных из и отправку данных 
в субгенератор.

**Чтение данных из генератора используя yield from**

```python
def reader():
    """
    Генератор, который создает видимость чтения из файла, сокета и т д
    """
    for i in range(4):
        yield '<< %s' % i

def reader_wrapper(g):
    # Ручной обход данных произведенных reader
    for v in g:
        yield v

wrap = reader_wrapper(reader())
for i in wrap:
    print(i)
# Result
<< 0
<< 1
<< 2
<< 3
```

Вместо ручного обхода данных reader() мы можем просто использовать yield from.

```python
def reader_wrapper(g):
    yield from g
```

Это работает и мы избавились от 1 строки кода.


**Отправка данных в генератор (сопрограмма) используя yield from. Часть 1.**

Сейчас давайте сделаем, что-нибудь более интересное. Создадим сопрограмму 
вызывающую `writer`, который принимает данные и отправляет их в сокет, 
файловый дескриптор и т д.

```python
def writer():
    """
    Сопрограмма, которая записывает данные в сокет, файловый дескриптор и т д
    """
    while True:
        w = (yield)
        print('>> ', w)
```

Теперь главный вопрос как должна выглядеть функция wrapper обрабатывающая 
отправку данных writer, так чтобы любые данные прозрачно передавались от 
`wrapper` к `writer()`.

```python
def writer_wrapper(coro):
    # Будет описана позже
    pass

w = writer()
wrap = writer_wrapper(w)
wrap.send(None)  # "prime" the coroutine
for i in range(4):
    wrap.send(i)

# Ожидаемый результат
>>  0
>>  1
>>  2
>>  3
```

Wrapper должен принять отправленные ему данные и так же должен обработать 
`StopIteration`, когда цикл `for` будет завершен. Очевидно, что просто написать

```python
for x in coro: 
    yield x
```

не получится.

Вот версия, которая работает:

```python
def writer_wrapper(coro):
    coro.send(None)  # prime the coro
    while True:
        try:
            x = (yield)  # Получаем значение, которое будем отправлять
            coro.send(x)  # передаем его для writer
        except StopIteration:
            pass
```

Или вместо этого мы можем написать это:

```python
def writer_wrapper(coro):
    yield from coro
```

Так мы сохраним 6 строчек кода, сделаем код более читаемым и это 
просто работает. Магия!


**Отправка данных в генератор (сопрограмма) используя yield from. Часть 2. 
Обработка ошибок.**

Давайте сделаем предыдущий пример немного сложнее. Что если нашему writer нужно 
обрабатывать исключения? Давайте скажем `writer` обрабатывать `SpamException` и 
выводить ***, если оно произошло.

```python
class SpamException(Exception):
    pass

def writer():
    while True:
        try:
            w = (yield)
        except SpamException:
            print('***')
        else:
            print('>> ', w)
```

Что если не менять `writer_wrapper`? Будет это работать? Давайте проверим!

```python
# writer_wrapper такой как выше (длинная версия)
w = writer()
wrap = writer_wrapper(w)
wrap.send(None)  # "prime" the coroutine

for i in [0, 1, 2, 'spam', 4]:
    if i == 'spam':
        wrap.throw(SpamException)
    else:
        wrap.send(i)


# Ожидаемый результат
>>  0
>>  1
>>  2
***
>>  4

# Полученный результат
>>  0
>>  1
>>  2
Traceback (most recent call last):
  ... redacted ...
  File ... in writer_wrapper
    x = (yield)
__main__.SpamException
```

Не работает, потому что x = (yield) просто возбуждает ошибку и вся программа 
аварийно завершается. Давайте заставим программу работать, будем ловить 
исключения в `writer_wrapper` и вручную кидать их в суб-генератор (`writer`), 
а если ошибки не возникает просто передавать значения в суб-генератор.

```python
def writer_wrapper(coro):
    """Works. Manually catches exceptions and throws them"""
    coro.send(None)  # prime the coro
    while True:
        try:
            try:
                x = (yield)
            except Exception as e:   # This catches the SpamException
                coro.throw(e)
            else:
                coro.send(x)
        except StopIteration:
            pass
```

Это работает.

```python
# Результат
>>  0
>>  1
>>  2
***
>>  4
```

Но и это тоже!

```python
def writer_wrapper(coro):
    yield from coro
```

`yield from` прозрачно обрабатывает отправку значений или кидает исключения 
внутрь суб-генератора.

Хотя это все равно не покрывает все возможные случаи. Что случиться если 
генератор будет закрыт (возникает `StopItaration`)? Что о том случае, если 
суб-генератор возвращает значение (в python 3 генератор может возвращать 
значение), как оно будет возвращаться? Все случаи, которые может обработать 
`yield` from описаны в PEP 380.

Ссылки:
- [Оригинальная статья](http://stackoverflow.com/questions/9708902/in-practice-what-are-the-main-uses-for-the-new-yield-from-syntax-in-python-3)
- [PEP 380 - Syntax for delegating to a sub-generator](http://legacy.python.org/dev/peps/pep-0380/)
- [PEP 342 - Coroutines via Enhanced Generators](http://legacy.python.org/dev/peps/pep-0342/)
- [Dave Beazley's Curious Course on Couroutines](http://dabeaz.com/coroutines/) 
данная статья великолепна для начала. 
[Посмотрите слайды 24-33](http://dabeaz.com/coroutines/Coroutines.pdf) чтобы 
увидеть наглядные примеры.


<a name='Классы'></a>
## Классы

<a name='Объектная-модель'></a>
### Объектная модель

<a name='Спец-атрибуты'></a>
#### Спец атрибуты

- `class.__bases__` - вернет кортеж базовых классов объекта
- `object.__dict__` - возвращает словарь атрибутов объекта


<a name='Реализация-доступа-по-индексу'></a>
#### Реализация доступа по индексу

Для того, чтобы реализовать у класса доступ по индексу. В нём нужно определить 
2 основных метода `__getitem__` и `__setitem__`.
`__getitem__` - отвечает за получение данных содержащихся по указанному индексу.
`__setitem__` - отвечает за присваивание объекту по индексу.

```python
class Test:
    def __init__(self, *args):
        self.data = list(args)

    def __getitem__(self, item):
        """
        Вызывается при доступе к атрибуту класса или по индексу
        """
        if isinstance(item, str):
            return getattr(self, item)
        else:
            return self.data[item]

    def __setitem__(self, key, value):
        """
        Вызывается при присваивании по индексу
        """
        self.data[key] = value

test = Test(1, 2)
print('Доступ атрибуту класса', test.data)
print('Доступ по индексу\n', test[1])

test[0] = 11
print('Присвоили данные по индексу', test.data)
test.data = ['новый', 'список']
print('Присвоили данные через атрибут класса', test.data)
```


<a name='Протокол-итераторов'></a>
#### Протокол итераторов

Работая с python Вы каждый день сталкиваетесь с необходимостью пробежаться по
всем строкам файла или перебрать все элементы списка, словаря или какой-то 
другой коллекции. Наверняка решаете Вы эту задачу простым циклом `for`.

```python
for i in [1, 2, 3]:
    print(i)
```

Вы можете так делать благодаря тому, что эти объекты реализуют протокол 
итераторов. Это значит что они реализуют интерфейс благодаря которому цикл 
`for` знает как их перебрать.

Для того, чтобы класс написаный вами тоже можно было использовать в цикле 
`for`, необходимо переопределить методы: `__iter__`, `__next__`.

Метод  `__iter__` вызывается циклом `for` в самом начале и возвращает 
итератор, при помощи которого можно последовательно перебрать все элементы
контейнера.

Итератор возвращенный методом `__iter__` должен реализовывать метод `__next__`,
который при каждом вызове будет возвращать следующий объект контейнера и так
до тех пор пока элементы не кончатся. Когда элементы кончились он должен 
возбудить исключение `StopIteration`, которое сведетельствует о том, что все 
элементы контейнера были перебраны.

Примеры:

```python
class MyList:

    def __init__(self, *args):
        self.data = args
        self.index = 0

    def __iter__(self):
        """
        Вызывается циклом for, в самом начале, чтобы получить объект,
        который реализует метод __next__, чтобы производить его обход.
        Так как данный класс реализует метод __next__, данный метод
        просто возвращает self.
        """
        return self

    def __next__(self):
        """
        Выдаёт циклу очередной элемент, а когда элементы заканчиваются,
        возбуждает исключение StopIteration
        """
        try:
            data = self.data[self.index]
            self.index += 1
        except IndexError:
            self.index = 0
            raise StopIteration
        return data

my_list = MyList(0, 1, 2, 3, 4)
for i in my_list:
    print(i)
```

Перебор всех атрибутов класса:

```python
class MyList:
    def __init__(self):
        self.atr1 = 0
        self.atr2 = 1
        self.atr3 = 'привет'
        self.atr4 = [1, 2, 3]

    def __iter__(self):
        """
        Вызывается циклом for, в самом начале, чтобы получить объект,
        который реализует метод __next__, чтобы производить его обход.
        Чтобы выполнять итерации над атрибутами класса используем 
        метод __dict__, который содержит словарь атрибутов класса. 
        Функция iter  добавляет в объект метод __next__, для
        поддержки итераций.
        """
        return iter(self.__dict__.values())

my_list = MyList()

for i in my_list:
    print(i)
```

<a name='Отличие-__getattr__-от-__getattribute__'></a>
#### Отличие `__getattr__` от `__getatrribute__`

**`__getattr__`**

Вызывается в тех случая, когда у класса пытаются запросить атрибут, который 
не определен ещё. По этому мы можем переопределить его и указать классу, что 
делать при доступе к несуществующему атрибуту.

```python
>>> class Foo:
...     some = 1
...     def __getattr__(self, item):
...         print('Атрибута "{}" не существует'.format(item))
... 
>>> foo = Foo()
>>> foo.some
1
>>> foo.some1
Атрибута "some1" не существует
```

**`__getattribute__`**

Вызывается при попытке доступа к атрибутам (существующим/не существующим).

```python
>>> class Foo:
...     some = 1
...     def __getattribute__(self, item):
...         print('Обращение к атрибуту "{}"'.format(item))
... 
>>> foo = Foo()
>>> foo.some
Обращение к атрибуту "some"
>>> foo.some1
Обращение к атрибуту "some1"
```

Если в классе одновременно определены оба эти метода, то `__getattr__` больше 
не будет вызываться. Вместо него будет использоваться `__getattribute__`.


<a name='MRO'></a>
#### MRO

- [MRO в Python, или Геометрия бриллианта](https://bartash.wordpress.com/2017/06/14/mro-in-python/)
- [Порядок разрешения методов в Python](https://habr.com/post/62203/)




<a name='Исключения'></a>
## Исключения

<a name='Извлечение-traceback'></a>
### Извлечение traceback

Иногда бывает нужно получить в виде строки полный traceback ошибки, это можно
сделать следующим способом:

```python
import traceback
       
def foo():
    a = 5 / 0

try:
    foo()
except Exception as err:
    print(traceback.format_exc())
```


<a name='Обработчик-для-не-перехваченных-исключений'></a>
### Обработчик для не перехваченных исключений

Пример ниже показывает как можно залогировать не перехваченные исключения.

```python
import sys
import logging

def unhandled_exception_hook(exc_type, exc_value, exc_traceback):
    logging.getLogger('app-name').error(
        '#CRITICAL Возникла непредвиденная ошибка в работе приложения:',
        exc_info=(exc_type, exc_value, exc_traceback)
    )

sys.excepthook = unhandled_exception_hook
```

<a name='Цепочки-исключений'></a>
### Цепочки исключений

Цепочки исключений это способ показать, что выбрасываемое исключение является
следствием неудачной попытки обработать другое исключение.

Для формирования цепочек исключений используется дополнение `from`. При его 
использовании требуется указать ещё одно исключение (класс, либо объект). Этот 
объект будет подвязан к возбуждаемому исключению в атрибут `__cause__` (атрибут 
поддерживает запись). В результате, если возбуждаемое исключение не обработано, 
то на вывод будут отправлены оба исключения:

```python
try:
    print(1 / 0)
except ZeroDivisionError as e:
    raise RuntimeError('Новое исключение') from e
```

Данный код говорит о том, что исключение `RuntimeError('Новое исключение')`
возникло в результате неудачной попытки обработать `ZeroDivisionError`.

Похожий механизм срабатывает автоматически, если исключение возбуждается 
внутри обработчика, либо блока `finally` — предыдущее исключение подвязывается 
в атрибут `__context__` нового:

```python
try:
    print(1 / 0)        
except:
    raise RuntimeError('Неявная цепочка исключений')
```




<a name='Сборщик-мусора'></a>
## Сборщик мусора

<a name='Как-работает-сборщик-мусора-(gc)'></a>
### Как работает сборщик мусора (gc)

Для очистки памяти в CPython используется сразу 2 способа:

- подсчет ссылок
- сборщик мусора (generational garbage collector он же gc)

Алгоритм подсчета ссылок очень простой и эффективный, но у него есть один 
большой недостаток. Он не умеет определять циклические ссылки. Именно из-за 
этого, в питоне существует дополнительный сборщик, именуемый поколенческий GC, 
который следит за объектами с потенциальными циклическими ссылками.

В Python, алгоритм подсчета ссылок является фундаментальным и не может 
отключен, тогда как GC опционален и может быть отключен.

**Подсчет ссылок**

Алгоритм подсчета ссылок это одна из самых простых техник для сборки мусора. 
Объекты удаляются как только на них больше нет ссылок.

В Python, переменные не хранят значения, а выступают в роли ссылок на объекты. 
То есть когда вы присваивайте значение новой переменной, то сначала создается 
объект с этим значением, а уже потом переменная начинает ссылаться на него. 
На один объект может ссылаться множество переменных.

Каждый объект в Python содержит дополнительное поле (счетчик ссылок), в 
котором хранится количество ссылок на него. Как только кто-то ссылается на 
объект, это поле увеличивается на единицу. Если по какой-то причине ссылка 
пропадает, то это поле уменьшается на один.

Примеры, когда количество ссылок увеличивается:

- оператор присваивания
- передача аргументов
- вставка нового объекта в лист (увеличивается количество ссылок для объекта)

Как только счетчик ссылок для определенного объекта достигает нуля 
интерпретатор запускает процесс уничтожения объекта. Если удаленный объект 
содержал ссылки на другие объекты, то эти ссылки также удаляются. Таким 
образом, удаление одного объекта может повлечь за собой удаление других. 

Например, если удаляется список, то счетчик ссылок во всех его элементах 
уменьшается на один. Если все объекты внутри списка больше нигде не 
используются, то они также будут удалены.

Переменные, которые объявлены вне функций, классов и блоков называются 
глобальными. Как правило, жизненный цикл таких переменных равен жизни Python 
процесса. Таким образом, количество ссылок на объекты на которые ссылаются 
глобальные переменные никогда не падает до нуля.

Переменные, которые объявлены внутри блока (функции, класса) имеют локальную 
видимость (т.е. они видны только внутри блока). Как только интерпретатор 
питона выходит из блока он уничтожает все ссылки созданные локальными 
переменными внутри него.

Вы всегда можете проверить количество ссылок используя 
функцию `sys.getrefcount.`

Пример работы счетчика ссылок:

```python
>>> import sys
>>> foo = 'Какая-то строка'
>>> print(sys.getrefcount(foo)) 
2
>>> bar = foo
>>> print(sys.getrefcount(foo)) 
3
>>> del bar
>>> print(sys.getrefcount(foo)) 
2
```

Данный пример наглядно показывает все то, о чем говорилось ранее. Небольшое 
недоумение может вызвать то, что нам вернулись значения 2, 3, 2, а не 1, 2, 1.
Это объясняется тем, что когда переменная передается в функцию,
то счетчик ссылок увеличивается на единицу (это сделано для того, чтобы объект
не был удален в то время, когда он обрабатывается функцией).

Основная причина, из-за которой стандартный интерпретатор (CPython) использует 
счетчик ссылок, является исторической. В настоящее время можно встретить 
множество дебатов по поводу данного подхода. Некоторые люди считают, что 
сборщик мусора может быть намного эффективней без участия алгоритма подсчета 
ссылок. У данного алгоритма есть множество проблем, таких как циклические 
ссылки, блокирование потоков, а также дополнительные накладные расходы на 
память и cpu.

Основным плюсом этого алгоритма является то, что объекты удаляются сразу как 
только они не нужны. 

**Garbage Collector**

Задача GC обрабатывать ситуации, когда у нас есть объекты у которых счетчик 
ссылок не 0, но они более не достижимы из кода. То есть он нужен только для
удаления объектов с циклическими ссылками, которые не были удалены алгоритмом
работающим по умолчанию и опирающимся на подсчете ссылок.
 
Ниже приведен пример визуализирующий проблему циклических ссылок, которую 
решает GC.

```python
import ctypes
import gc

# выключаем GC
gc.disable()


# используется ctypes для доступа к объектам по адресу памяти
class PyObject(ctypes.Structure):
    _fields_ = [("refcnt", ctypes.c_long)]


object_1 = {}
object_2 = {}
object_1['obj2'] = object_2
object_2['obj1'] = object_1

obj_address = id(object_1)

# Удаляем ссылки, таким образом объекты более недоступны в программе, 
# но счетчик ссылок у них не 0
del object_1, object_2

# раскомментируйте для принудительного запуска gc, чтобы убедится, что он 
# удалит недостижимые объекты
# gc.collect()

# проверяем счетчик ссылок
print(PyObject.from_address(obj_address).refcnt)
```

В примере выше, инструкция del удаляет ссылки на наши объекты (не сами 
объекты). Как только Python выполняет инструкцию del эти объекты становятся 
недоступны из Python кода. Однако, с выключенным модулем gc они по прежнему 
будут оставаться в памяти, т.к. они имели циклические ссылки и их счетчик по 
прежнему равен единице. Вы можете визуально исследовать такие связи используя 
библиотеку objgraph. 

Циклические ссылки могут происходить только в “контейнерных” объектах. Т.е. 
в объектах, которые могут хранить другие объекты, например в списках, словарях,
классах и кортежах. GC не следит за простыми и неизменяемыми типами, за 
исключением кортежей. Некоторые кортежи и словари так же исключаются из списка 
слежки при выполнении определенных условий. Со всеми остальными объектами 
гарантированно справляется алгоритм подсчета ссылок.

**Поколения**

Поиск недостижимых из кода программы объектов может занимать очень много 
времени, по этому все объекты разделяются на 3 поколения (0, 1, 2), чем дольше
живет объект, тем в более старшем поколении он будет находится. Поколения 
представляют из себя обычные списки, с которыми работает gc.

Алгоритм распределения по поколениям:

- в поколение 0 попадают все новые объекты, кроме тех, которые НЕ могут 
ссылаться на другие объекты (примерами таких объектов могут служить объекты 
типов str, int and etc);
- в 1 поколение попадают те объекты, которые не были удалены во время нулевой
чистки;
- во 2 поколения попадают объекты пережившие чистку в поколении 1.

Момент запуска GC определяется при помощи специальных значений называемых 
threshold. Для каждого поколения имеется свое знанчение threshold. Посмотреть 
эти значения можно вот так:

```python
>>> import gc
>>> gc.get_threshold()
(700, 10, 10)
```

Таким образом GC запускается:
- для поколения 0 - когда выполняется следующие условие:

    (количесто созданных объектов - количество удаленных объектов) > 700

- для поколения 1 - после того как GC 10 раз отработал для поколения 0
- для поколения 2 - после того GC 10 раз раз отработал для поколения 1

Значения threshold можно изменять при помощи функции `gc.set_threshold`.

**Ограничения GC**

GC не может корректно обрабатывать объекты с переопределенным деструктором 
(`__del__`), потому что он просто не знает в каком порядке их вызывать, чтобы 
не наткнуться в очередном деструкторе на вызов уже удаленного объекта. По этому
объекты с переопредленным деструктором, а так же другие объекты, которые gc
не знает как удалить, он помещает в `gc.garbage`. 

Для того, чтобы очистить этот список, нужно перебрать все элементы этого списка
и разорвать все циклические ссылки (`gc.get_referrers`). После этого данный 
список можно очистить и тогда python сам удалит эти объекты основываясь на
посчете ссылок.

P.S. учитывая сложность и множество нюансов работы с `__del__`, лучше не 
использовать их.

Стоить отметить, что данная проблема была решена в Python 3.4 
(подробнее в [PEP 442](http://legacy.python.org/dev/peps/pep-0442/))

**Советы по оптимизации**

Циклы зачастую случаются в реальных задачах, их можно встретить в задачах с 
графами, связными списками или в структурах данных, где требуется вести учёт 
отношений между объектами. Если ваша программа имеет высокую нагрузку и 
требовательна к задержкам, то, по возможности, циклы лучше избегать. 

В местах, где вы заведомо используйте циклические ссылки, можно использовать 
«слабые» ссылки. Слабые ссылке реализованы в модуле weakref и в отличие от 
обычных ссылок никак не влияют на счётчик ссылок. Если объект со слабой ссылок 
оказывается удалённым, то вместо него возвращается None.

В некоторых случаях полезно отключить автоматическую сборку модулем gc и 
вызывать его вручную. Для этого достаточно вызывать gc.disable() и в 
дальнейшем вызывать gc.collect() вручную.


Материалы используемые для написания заметки:

- [Garbage collector & a bit of memory management - Кирилл Лашкевич - Python Meetup 27.09.2013](https://www.youtube.com/watch?v=6Iw6n43mrVI)
- [Всё, что нужно знать о сборщике мусора в Python - Habr](https://habr.com/post/417215/)
- [Удаление объектов сборщиком мусора - asvetlov](https://asvetlov.blogspot.com/2013/05/gc.html)
- [Подсчет ссылок и сборка мусора в Python](http://python-3.ru/page/podschet-ssylok-i-sborka-musora-v-python)



<a name='Многозадачность'></a>
## Многозадачность

<a name='Сравнение-асинхронности-и-многопоточности-Описание-роли-GIL-во-всем-этом'></a>
### Сравнение асинхронности и многопоточности. Описание роли GIL во всем этом.

Не один раз видел, что не все программисты на Python понимают, в каких случаях 
стоит создавать отдельный процесс, а когда можно обойтись потоком. Да и к 
тому-же, сейчас многие могут думать, что AsyncIO — самый правильный способ 
писать веб-приложения. Я попробую объяснить разницу между этими формами 
многозадачности в Python и когда их лучше применять.

Задачи в компьютере делятся на два вида:

**CPU Bound** — операции, задействующие центральный процессор. Как правило, это 
вычисления: работа с матрицами, изображениями, анализ больших массивов данных, 
вычисление последовательности Фибоначчи или майнинг биткоинов.

**I/O Bound** — задачи, использующие ввод/вывод: работу с диском или сетью. К 
таковым относятся веб-сервера или часть веб-приложения, которая принимает 
запросы от клиентов.

**Процессы и потоки**

Процесс — программа, которая запущена в данный момент. С точки зрения ОС, 
процесс — это структура данных, за которой закреплена область памяти и 
некоторые другие ресурсы, например, открытые им файлы.

Потоки, они же треды или нити — единица исполнения внутри процесса. Часто у 
процесса один поток, называемый основным, но по желанию, программа может 
создать любое их количество. При старте потоку не выделяется отдельных 
ресурсов, вместо этого он использует память и ресурсы породившего его процесса.
За счет этого потоки быстро стартуют и останавливаются.

Обеспечением многозадачности занимается планировщик — часть ядра ОС, которая по
очереди загружает потоки исполнения в центральный процессор.

На одном ядре процессора в единицу времени выполняется один поток. Он работает 
до тех пор, пока не израсходует свой квант времени (по умолчанию равен 100 мс) 
или сам не уступит управление следующему потоку, совершив системный вызов. 
В Python каждый процесс и поток — это нативный процесс и поток операционной 
системы, так что, для него эти утверждения так-же верны.

Но в эталонной реализации Python — CPython присутствует печально известный GIL 
(Global Interpreter Lock), по сути, глобальный семафор, который не дает 
одновременно работать больше чем одному потоку в рамках процесса интерпретатора.

**Несколько фактов о GIL**

- GIL защищает структуры данных работающего потока от проблем конкурентного 
доступа. Например, предотвращает состояние гонки при изменении значения счетчика
ссылок объекта.

- GIL упрощает интеграцию non thread safe библиотек на С. Благодаря GIL у нас 
так много быстрых модулей и биндингов почти ко всему.

- Библиотекам на C доступен механизм управления GIL. Так например NumPy 
отпускает его на долгих операциях.

Когда поток начинает работу, он выполняет захват GIL. Спустя какое-то время 
планировщик процессов решает, что текущий поток поработал достаточно, и 
передает управление следующему потоку. Поток №2 видит, что GIL захвачен, так 
что он не продолжает работу, а погружает себя в сон, уступая процессор потоку №1.

Но поток не может удерживать GIL бесконечно. До Python 3.3 GIL переключался 
каждые 100 инструкций машинного кода. В поздних версиях GIL может быть удержан 
потоком не дольше 5 мс. GIL так-же освобождается, если поток совершает 
системный вызов, работает с диском или сетью.

По сути, GIL в питоне делает бесполезной идею применять потоки для параллелизма
в вычислительных задачах. Они будут работать последовательно даже на 
многопроцессорной системе. На CPU Bound задачах программа не ускорится, а 
только замедлится, так как теперь потокам придется делить пополам процессорное
время. При этом I/O операции GIL не замедлит, так как перед системным вызовом
поток отпускает GIL.

На этой грустной ноте можно придти к выводу, что для распараллеливания задач, 
которые завязаны на ввод/вывод хватит и тредов. А вот вычислительные задачи 
следует запускать в отдельных процессах.

**Сопрограммы и AsyncIO**

Теперь представим, что мы пишем HTTP или WebSocket сервер, который каждое 
подключение обрабатывает в отдельном потоке.

Здесь вполне можно создать 100, может даже 500 потоков, чтобы обработать 
нужное количество одновременных соединений. Для коротких запросов это даже 
будет работать и позволит выдержать нагрузку в 5000 RPS на самом дешевом 
инстансе в DO за пять баксов — вполне неплохо. Если у вас меньше, возможно 
здесь и не нужны никакие AsyncIO/Tornado/Twisted.

Но что, если их количество стремится к бесконечности? Скажем, это большой чат 
с кучей каналов, где количество одновременных участников не ограничено. В 
такой ситуации создать столько потоков, чтобы хватило каждому пользователю я 
бы уже не рискнул. И вот почему:

Как говорилось выше, пока GIL захвачен одним потоком, другие работать не будут.
Планировщик операционной системы, при этом, о GIL ничего не знает и все равно 
будет отдавать процессор заблокированный потокам. Такой поток, конечно, увидит 
что GIL захвачен и сразу же уснет, но на переключение контекста процессора 
будет тратиться драгоценное время.

Переключение контекста — вообще дорогая для процессора операция, которая 
требует сброса регистров, кэша и таблицы отображения страниц памяти. Чем больше
потоков запущено, тем больше процессор совершает холостых переключений на 
потоки, заблокированные GIL, прежде чем дойдет до того самого, который этот 
GIL удерживает. Не очень-то эффективно.

Есть старые добрые сопрограммы — то, что сейчас предлагает AsyncIO и Tornado. 
Их еще называют корутинами или просто потоками на уровне пользователя. Модная 
нынче штука, но, далеко не новая, а использовалась еще во времена, когда в ходу
были ОС без поддержки многозадачности.

В отличие от потоков, сопрограммы выполняют только полезную работу, а 
переключение между ними происходит только в тот момент, когда сопрограмма 
ожидает завершения какой-то внешней операции.

```python
import asyncio
import aiohttp

async def simulate_io():
    async with aiohttp.ClientSession() as session:
        async with session.get('http://python.org') as resp:
            await resp.text()

async def coro(name):
    await simulate_io()
    print(f'{name}-1')
    await simulate_io()
    print(f'{name}-2')

async def main():
    await asyncio.gather(coro('A'), coro('B'))

loop = asyncio.get_event_loop()
loop.run_until_complete(main())
```

Результаты получим примерно в таком порядке: A-1 B-1 A-2 B-2. Из примера видно,
что сопрограммы работали по очереди, при этом все происходит в одном потоке. 
Когда сопрограмма A вызывает `simulate_io()`, управление передаётся сопрограмме 
B. Она делает тот-же вызов и управление возвращается сопрограмме A, которая 
печатает результат, так как I/O для неё уже завершился и переходит ко 
следующему вызову `simulate_io()`.

Как и в случае с тредами, асинхронщина бесполезна для вычислений. Тут ситуация 
даже хуже, так как зависший на вычислениях поток рано или поздно GIL отпустит, 
а вот блокирующий код в сопрограмме заблокирует весь поток, до тех пор, пока не
исполнится весь. В отличии от нативных тредов, у сопрограмм отсутствует 
прерывание по таймеру. Передача управления следующей сопрограмме происходит 
вручную, при явном вызове конструкции await (или yield, если используются 
generator-based корутины). Поэтому важно следить, чтобы в асинхронных 
программах не было блокирующего кода и использовались только асинхронные 
вызовы, а все вычисления происходили в отдельных процессах.

```python
def fib(n):
    if n <= 2:
        return 1
    return fib(n - 1) + fib(n - 2)

async def coro(name):
    await simulate_io()
    print(f'{name}-1')
    fib(35)  # здесь весь поток блокируется
    await simulate_io()
    print(f'{name}-2')
```

**Итого**

Потоки будут проще, если у вас типичное веб-приложение, которое не зависит от 
внешних сервисов, и относительно конечное количество клиентов, для которых 
время ответа будет предсказуемо-коротким.

AsyncIO подойдет, если приложение большую часть времени тратит на чтение/запись
данных, а не их обработку. Например, у вас много медленных запросов — 
вебсокеты, long polling или есть медленные внешние синхронные бекенды, запросы
к которым неизвестно когда завершатся.

Ссылки по теме:

- [Асинхронный Python и различные формы многозадачности](https://maxpoletaev.ru/blog/async-python/) - отсюда 
 взят текст статьи
- [GIL в Python: зачем он нужен и как с этим жить - Григорий Петров](https://www.youtube.com/watch?v=AWX4JnAnjBE) - в 
 данном видео текст заметки раскрыт гораздо лучше


<a name='В-чем-отличие-конкурентности-от-многопоточности'></a>
### В чем отличие конкурентности от многопоточности

Отличие конкуретного подхода от многопоточного, заключается в том, что 
потоками управляет ОС и она производит их переключение по своему алгоритму,
на который нельзя повлиять, а сопрограммами управляет event loop, который 
производит переключение в "правильный" момент времени, то есть когда для 
коррутины появилась работа (на пример пришли данные по сети).

Ссылки на использованные материалы:

- [Difference between a “coroutine” and a “thread”? - stackoverflow.com](https://stackoverflow.com/questions/1934715/difference-between-a-coroutine-and-a-thread)
 

<a name='Асинхронность-в-python'></a>
### Асинхронность в python

[Алексей Кузьмин (Domclick) - Асинхронное программирование в Python](https://www.youtube.com/watch?v=OEFsdk1tqAU), 
[копия](https://cloud.mail.ru/public/59GN/36QyzVCXE), [презентация](https://cloud.mail.ru/public/3TxW/2e2487e6i),
[скрипты с доклада](https://github.com/fantomius/AsyncPython/tree/master) - хороший 
доклад рассказывающий о том, как реализована асинхронность в python. Объяснение 
ведется на примерах, которые показывают как написать асинхронный код не 
используя библиотеки вроде aiohttp.

[Алексей Кузьмин (ДомКлик) - Асинхронность изнутри](https://www.youtube.com/watch?v=pZkerqks43Y),
[копия](https://cloud.mail.ru/public/3jm2/3NGxXo5Q9) - продолжение 
предыдущего доклада, в котором подробно на простых примерах рассказывается, что
из себя представляет асинхронный код. Хорошо рассказаны все основы.

[Алексей Кладов - async / await (2018) (Computer Science Center)](https://www.youtube.com/watch?v=x6JZmBK2I8Y),
[копия](https://cloud.mail.ru/public/2irA/2SUoGXmYD)

[Асинхронный Python-код медленнее обычного кода! Ааа!!1один. Aiohttp VS синхронные фреймворки - Диджитализируй!](https://www.youtube.com/watch?v=z7WIm0iZcOU),
[копия](https://cloud.mail.ru/public/mYPT/XU8pWyPE4) - автор разбирает один из бенчмарков в котором сравнивается 
производительность асинхронного и синхронного кода, объясняет почему получились те или иные значения и подробно 
рассказывает для чего хорош асинхронный код, а для чего синхронный.


<a name='Оптимизация-и-профилирование'></a>
## Оптимизация и профилирование

<a name='Способы-измерения-производительности'></a>
### Способы измерения производительности

<a name='Измерение-производительности-отдельных-функций'></a>
#### Измерение производительности отдельных функций

Измерить производительность отдельных функций (их время выполнения) можно при 
помощи функции timeit из модуля timeit.

```python
import timeit

def test1():
    res = [[3 for x in range(50)] for x in range(50)]

print(timeit.timeit("test1()", setup="from __main__ import test1", number=100000))
```

Первым аргументом функция timeit принимает код, продолжительность работы 
которого нужно измерить. Второй аргумент — это инструкция, которая выполняется 
1 раз, для настройки окружения. Третий агрумент, задаёт число повторных 
запусков тестируемого кода.


<a name='Замер-времени-работы-с-помощью-unix-программы-time'></a>
#### Замер времени работы с помощью unix программы time

Замерять время выполнения написанной программы можно с помощью стандарной 
Unix программы time (не нужно путать с одноименной программой bash). 

Пример: 

```bash
time python3 get_email.py
```

Вывод программы: 

- real 0m2.784s - реальное время выполнения между вызовом и завершением
- user 0m0.054s - время CPU, которое занял пользователь (сумма значений 
tms_utime и tms_cutime в структуре struct tms, которая возвращается 
вызовом times)
- sys  0m0.010s - время CPU занятое системой (сумма значений tms_stime и 
tms_cstime в структуре struct tms, которая возвращается вызовом times)


<a name='Примеры-написания-оптимального-кода'></a>
### Примеры написания оптимального кода

<a name='Создание-списков-заполненных-определённым-числом'></a>
#### Создание списков заполненных определённым числом

Есть задача: «Создать список 50*50 целых чисел и заполненный тройками.».

Её можно решить несколькими способами:

- Использовать генераторы списков для создания и заполнения списка тройками
    ```python
    [[3 for x in range(50)] for x in range(50)]
    ```
    такой способ будет работать, но как показали замеры, работает он 
    достаточно медленно 9.265034271054901.
- Использование операции умножения для получения списка
    ```python
    [[3] * 50 for _ in range(50)]
    ```
    как показали тесты, данный способ работает значительно быстрее 1.407502235029824.
    

<a name='Добавление-данных-в-список'></a>
#### Добавление данных в список

Добавление большого колличесва данных в список, это распространненая задача и 
решать ее можно по разному. Вот некоторые из вариантов, которые могут 
использоваться:

- в цикле добавлять данные при помощи метода append
- использовать генератор списков

Из перечисленных выше вариантов быстрее всего будет отрабатывать генератор
списков и если есть возможность, то лучше использовать его.

Если ситуация не позволяет (на пример перед добавлением должна выполнится 
дополнительная логика), то необходимо использовать следующую оптимизацию:

```python
res = []
append = res.append
for i in range(10):
    append(input('Введите данные: '))
```

В данном примере мы сохраняем в локальную переменную ссылку на метод `append`
и потом сразу используем эту ссылку сразу, а не вызываем каждый раз данный метод
у объекта res. Данный подход позволяет нам сэкономить драгоценное время, так 
как python не будет каждый раз искать у объекта метод `append`.

При написании кода лучше всегда помнить про подобного рода трюк, потому что в 
python lookup`ы в python очень "дорогие".


<a name='Советы-по-написанию-оптимального-кода'></a>
### Советы по написанию оптимального кода

<a name='Мелкие-классы'></a>
#### Мелкие классы

Создание функций более дешёвая операция по сравнению с созданием экземпляра 
класса. По этому если в коде очень часто приходится создать экземпляр 
какого-то класса, нужно подумать как можно переписать этот код. Можно 
постараться избавиться от создания лишних экземпляров класса или вообще 
переписать этот участок кода в функциональном стиле.

По этой же причине не рекомендуется плодить у себя в проекте кучу маленьких 
классов, реализующих буквально несколько методов.

Полезные ссылки:

- [Перестаньте писать классы](https://habr.com/ru/post/140581/)


<a name='Поиск-(lookup)-очень-дорогой'></a>
#### Поиск (lookup) очень дорогой

Операции поиска:

- локальных/глобальных переменных
- замыкания
- атрибутов/методов

Это ОЧЕНЬ ДОРОГИЕ операции и 90% времени python занимается именно этим, по 
этому нужно быть с этим очень осторожным.

Для оптимизации кода, нужно запомнить в локальные переменные все, что нам нужно.

```python
def list_create():
    res = []
    __append = res.append
    for i in range(10):
        __append(input('Введите данные: '))
```

Так как мы 1 раз нашли метод append и сохранили его в свою локальную 
переменную, то в цикле мы не выполняем многократно дорогостоящий поиск, а 
вызываем нужный метод из локальной переменной.

Если наша функция list_create должна выполняться много раз, то глупо будет 
постоянно инициализировать наши локальные переменные, предназначены для 
запоминания часто используемых функциий. Для оптимизации этого момента можно 
использовать coroutines.

```python
def coroutine(func):
    def wrapper(*args, **kwargs):
        gen = func(*args, **kwargs)
        next(gen)
        return gen
    return wrapper

@coroutine
def list_create():
    args = ()
    res = []
    __append = res.append
    __clear = res.clear
    while True:
        __clear()
        for i in args:
            __append(i)
        args = yield res
        
list_create_cor = list_create()
print(list_create_cor.send(('foo', 'spam', 'list')))
```

Полезные ссылки:

- [Иван Ремизов - Сверхоптимизация кода на Python](https://youtu.be/4CsOOfdoU2A), [Копия](https://cloud.mail.ru/public/35XE/2AcgrAM3a)


<a name='Ввод/вывод-(python-3.X)'></a>
#### Ввод/вывод (python 3.X)

Функция `input` реализует дополнительную логику, по этому если нужно считывать 
данные со стандартного ввода и делать это очень быстро, то лучше напрямую 
обращаться к `sys.stdin`.

Считывать c stdin можно с помощью 2 методов:

- `read()` - читает данные до тех пор, пока не будет получен EOF (обычно это Ctrl+D)
- `readline()` - читает до тех пор, пока не будет нажат Entre.

**Пример задачи, где это может пригодиться:**

Ограничения: 1.5 секунды, 64 МБ памяти
На стандартный ввод программе сначала передаётся количество чисел, которые 
нужно считать, потом происходит ввод этих чисел. После этого опять передаётся 
количество чисел, которые нужно будет считать и после последовательно вводятся
эти числа.

Вы должны вывести одно число — количество чисел во втором списке, которые 
также содержатся в первом.

Пример входных данных:

|Входные данные|Результат|
|--------------|---------|
|2             |2        |
|1054          |         |
|1492          |         |
|4             |         |
|1492          |         |
|65536         |         |
|1492          |         |
|100           |         |


Решение с использованием `input`

```python
required_size = int(input())
required = {input() for i in range(required_size)}

written_size, result = int(input()), 0
for i in range(written_size):
    result += input() in required
 
print(result)
```

Решение с использованием прямого чтения из `sys.stdin`

```python
import sys
 
required_size = int(sys.stdin.readline())
required = {sys.stdin.readline() for i in range(required_size)}
 
written_size, result = int(sys.stdin.readline()), 0
for i in range(written_size):
    result += sys.stdin.readline() in required
 
print(result)
```

Если решать в лоб и использовать input(), то программа выполнится за 1.528, 
а если на прямую читать с sys.stdin, то программа выполнится за 0.858.


<a name='Оптимизация-потребления-памяти,-способы-поиска-утечек-памяти'></a>
#### Оптимизация потребления памяти, способы поиска утечек памяти

Материалы по теме:

- [«Память и Python. Что надо знать для счастья?» Алексей Кузьмин, ЦНС](https://www.youtube.com/watch?v=D0vbuIDOV4c), 
  [Локальная копия доклада](https://cloud.mail.ru/public/485j/5dzHRTWb8),



<a name='Deploy'></a>
## Deploy


<a name='Публикация-библиотеки-в-PyPi'></a>
### Публикация библиотеки в PyPi

Опубликовать библиотеку в pypi очень просто, а если в проекте используется 
poetry, то и не понадобится устанавливать никаких дополнительных утилит.

Перед тем как спешить опубликовать библиотеку в PyPi нужно знать, что 
после загрузки опубликованную библиотеку нельзя будет исправить и загрузить 
повторно не подняв версию. Поэтому к публикации нужно подходить с умом и 
действовать осторожно, а если необходимо произвести пробную загрузку, то можно 
воспользоваться тестовой средой, которая доступна 
по [ссылке](https://test.pypi.org).

Подготовка к загрузке:

- перейдите в корень проекта и добавьте ссылки на репозитории PyPi

    ```
    poetry config repositories.pypi https://upload.pypi.org/legacy/
    poetry config repositories.test_pypi https://test.pypi.org/legacy/
    ```

- получите токен, позволяющий работать с API PyPi. Если токен ранее не 
  создавался, то создать его можно, перейдя в 
  [настройки аккаунта](https://pypi.org/manage/account/), в разделе `API tokens`.


Сборка и публикация:

1. Собираем библиотеку `poetry build`

2. Загружаем собранную библиотеку в тестовый репозиторий

    ```
    poetry publish -r test_pypi -u __token__ -p <токен>
    ```
       
    > При авторизации, при помощи токена, логин всегда будет `__token__`.
       
    После загрузки найти библиотеку можно [тут](https://test.pypi.org/manage/projects/).
       
    Если загрузка прошла успешно, можно приступать к загрузке в основной репозиторий:
       
    ```
    poetry publish -r pypi -u __token__ -p <токен>
    ```
       
    Загруженную библиотеку можно найти [здесь](https://pypi.org/manage/projects/).



<a name='Тестирование'></a>
## Тестирование

<a name='Основы-тестирования'></a>
### Основы тестирования

- [Вадим Пуштаев - Юнит-тесты в проектах Поиска mail.ru](https://www.youtube.com/watch?v=npJVGhmQSU4), [Копия](https://cloud.mail.ru/public/4oGW/gWYu8tKNV)


<a name='Mock'></a>
### Mock

Работа с моками не всегда тривиальна и иногда нужна шпаргалка, которая поможет вспомнить как реализовать тот или иной 
распространенный кейс. Для этого, я создал простой проект, в котором на примитивных примерах показываю как использовать
ту или иную функциональность моков. Вы можете ознакомиться с ним по [ссылке](https://gitlab.com/alex925/mock-example).

Полезные ссылки:

- [Модуль Mock: макеты-пустышки в тестировании - habr](https://habr.com/ru/post/141209/)
- [Mock a class and a class method in python unit tests - stackoverflow](https://stackoverflow.com/questions/33585431/mock-a-class-and-a-class-method-in-python-unit-tests)
- [Unit-тесты, пытаемся писать правильно, чтобы потом не было мучительно больно - habr](https://habr.com/ru/post/116372/)



<a name='Разное'></a>
## Разное

<a name='Работа-с-датой-и-временем'></a>
### Работа с датой и временем

Стандартная библиотека имеет не до конца продуманное API работы с датой и 
временем, именно по этому была создана библиотека
[arrow](https://arrow.readthedocs.io/en/latest/). Она представляет из себя 
замену стандартного модуля datetime. 

Если в вашем приложении необходимо много 
работать с датой и временем, то рекомендуется использовать именно эту 
библиотеку.

Для того, чтобы узнать подробности о проблемах работы с датой и временем
прочитайте следующие статьи:

- [Питон: времена, даты и временные зоны - Андрей Светлов](https://asvetlov.blogspot.com/2011/02/date-and-time.html), [локальная копия](https://cloud.mail.ru/public/2DTf/245vSkJmp)
- [Часовые пояса - документация django](https://djbook.ru/rel1.9/topics/i18n/timezones.html)
- [Работа с датой и временем](../computer-science/computer-science-index.md#Работа-с-датой-и-временем)
